{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"max-width:66ch;\">\n",
    "\n",
    "# Lecture notes - Regularized linear models\n",
    "\n",
    "This is the lecture note for **regularized linear models**\n",
    "\n",
    "<p class = \"alert alert-info\" role=\"alert\"><b>Note</b> that this lecture note gives a brief introduction to regularized linear models. I encourage you to read further about regularized linear models. </p>\n",
    "\n",
    "Read more:\n",
    "\n",
    "- [Regularized linear models medium](https://medium.com/analytics-vidhya/regularized-linear-models-in-machine-learning-d2a01a26a46)\n",
    "- [Ridge regression wikipedia](https://en.wikipedia.org/wiki/Ridge_regression)\n",
    "- [Tikhonov regularization wikipedia](https://en.wikipedia.org/wiki/Tikhonov_regularization)\n",
    "- [Lasso regression wikipedia](https://en.wikipedia.org/wiki/Lasso_(statistics))\n",
    "- [Korsvalidering](https://sv.wikipedia.org/wiki/Korsvalidering)\n",
    "- [Cross validation](https://machinelearningmastery.com/k-fold-cross-validation/)\n",
    "- [Scoring parameter sklearn](https://scikit-learn.org/stable/modules/model_evaluation.html)\n",
    "- [ISLRv2 pp 198-205](https://www.statlearning.com/)\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "import pandas as pd "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"max-width:66ch;\">\n",
    "\n",
    "## Data preparation\n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((134, 19), (66, 19), (134,), (66,))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "df = pd.read_csv(\"../data/Advertising.csv\", index_col=0)\n",
    "X, y = df.drop(\"Sales\", axis = 1), df[\"Sales\"]\n",
    "\n",
    "# in exercise 2 Polynomial regression you've found the elbow in degree 4, as the error increases after that\n",
    "# however to be safe and we assume that the model shouldn't have too many interactions between different features, I will choose 3\n",
    "# please try with 4 and see how your evaluation score differs\n",
    "model_polynomial = PolynomialFeatures(3, include_bias=False)\n",
    "poly_features = model_polynomial.fit_transform(X)\n",
    "\n",
    "# important to not forget \n",
    "X_train, X_test, y_train, y_test = train_test_split(poly_features, y, test_size=0.33, random_state=42)\n",
    "\n",
    "# from 3 features we've featured engineered 34\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"max-width:66ch;\">\n",
    "\n",
    "## Feature standardization\n",
    "Remove sample mean and divide by sample standard deviation \n",
    "\n",
    "$X' = \\frac{X-\\mu}{\\sigma}$\n",
    "\n",
    "LASSO, Ridge and Elasticnet regression that we'll use later require that the data is scaled.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Control that mean is 0 and standard deviation 1\n",
      "Scaled X_train mean -0.00, std 1.00\n",
      "Note here that this is correct, as we don't have data leakage\n",
      "Scaled X_test mean -0.12, std 1.12\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaled_X_train = scaler.fit_transform(X_train)\n",
    "scaled_X_test = scaler.transform(X_test)\n",
    "\n",
    "print(\"Control that mean is 0 and standard deviation 1\")\n",
    "print(\n",
    "    f\"Scaled X_train mean {scaled_X_train.mean():.2f}, std {scaled_X_train.std():.2f}\"\n",
    ")\n",
    "print(\"Note here that this is correct, as we don't have data leakage\")\n",
    "print(f\"Scaled X_test mean {scaled_X_test.mean():.2f}, std {scaled_X_test.std():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"max-width:66ch;\">\n",
    "\n",
    "## Regularization techniques\n",
    "\n",
    "Problem with overfitting was discussed in previous lecture. When model is too complex, data noisy and dataset is too small the model picks up patterns in the noise. The output of a linear regression is the weighted sum: \n",
    "$y = w_0 + w_1x_1 + w_2x_2 + \\ldots + w_nx_n$ , where the weights $w_i$ represents the importance of the $ith$ feature. Want to constrain the weights associated with noise, through regularization. We do this by adding a regularization term to the cost function used in training the model. Note that the cost function for evaluation now will differ from the training.\n",
    "\n",
    "<p class = \"alert alert-info\" role=\"alert\"><b>Note</b> most regularization model requires scaling of data </p>\n",
    "\n",
    "### Ridge regression \n",
    "Also called Tikhonov regularization or $\\ell_2$ regularization has a penalty term $\\lambda \\ge 0$, which reduces variance by increasing bias. This means that the higher penalty is, the more we punishes weights $w_1, w_2, \\ldots, w_n$, causing the model to become less complex. \n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.5148267621786812, 0.37485164412180333)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "def ridge_regression(X, penalty=0):\n",
    "    # alpha = 0 should give linear regression\n",
    "    # note that alhpa is same as lambda in theory, i.e. penalty term. sklearn has chosen alpha to generalize their API\n",
    "    model_ridge = Ridge(alpha=penalty) \n",
    "    model_ridge.fit(scaled_X_train, y_train)\n",
    "    y_pred = model_ridge.predict(X)\n",
    "    return y_pred\n",
    "\n",
    "\n",
    "y_pred = ridge_regression(scaled_X_test, 0)\n",
    "MSE = mean_squared_error(y_test, y_pred)\n",
    "RMSE = np.sqrt(MSE)\n",
    "\n",
    "RMSE, mean_absolute_error(y_test, y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.5148267621786576, 0.3748516441217811)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check with linear regression -> RMSE very similar!\n",
    "from sklearn.linear_model import LinearRegression\n",
    "model_linear = LinearRegression()\n",
    "model_linear.fit(scaled_X_train, y_train)\n",
    "y_pred = model_linear.predict(scaled_X_test)\n",
    "np.sqrt(mean_squared_error(y_test, y_pred)), mean_absolute_error(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"max-width:66ch;\">\n",
    "\n",
    "### Lasso regression \n",
    "Similar to ridge regression, lasso has penalty which increases bias and decreases variance. However lasso will effectively set least important features to zero.\n",
    "\n",
    "It sets least important features to zero, when $\\lambda$ sufficiently large. This is practically feature selection. \n",
    "\n",
    "Note that in Lasso regression it is important to have scaled your data\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7853962108799017, 0.5735346450114956)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "model_lasso = Lasso(alpha = .1)\n",
    "model_lasso.fit(scaled_X_train, y_train)\n",
    "y_pred = model_lasso.predict(scaled_X_test)\n",
    "np.sqrt(mean_squared_error(y_test, y_pred)), mean_absolute_error(y_test, y_pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"max-width:66ch;\">\n",
    "\n",
    "### k-fold cross-validation\n",
    "\n",
    "One strategy to choose the best hyperparameter alpha is to take the training part of the data and \n",
    "1. shuffle dataset randomly\n",
    "2. split into k groups\n",
    "3. for each group -> take one test, the rest training -> fit the model -> predict on test -> get evaluation metric\n",
    "4. take the mean of the evaluation metrics\n",
    "5. choose the parameters and train on the entire training dataset\n",
    "\n",
    "Repeat this process for each alpha, to see which yielded lowest RMSE. k-fold cross-validation: \n",
    "- good for smaller datasets\n",
    "- fair evaluation, as a mean of the evaluation metric for all k groups is calculated\n",
    "- expensive to compute as it requires k+1 times of training\n",
    "\n",
    "### Ridge regression CV\n",
    "\n",
    "In scikit-learn, the implementation of cross validation is given through using the algorithm with CV in the ending, e.g. RidgeCV\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import RidgeCV # ridge regression with cross-validation\n",
    "\n",
    "# negative because sklearn uses convention of higher return values are better\n",
    "model_ridgeCV = RidgeCV(alphas = [.0001, .001, .01, .1, .5, 1, 5, 10], scoring = \"neg_mean_squared_error\")\n",
    "model_ridgeCV.fit(scaled_X_train, y_train)\n",
    "print(model_ridgeCV.alpha_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.5635899169556632, 0.4343075766484079)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# best alpha is 0.1\n",
    "# it seems that linear regression outperformed ridge regression in this case\n",
    "# however it could depend on the distribution of the train|test data, so using alpha = 0.1 is more robust here\n",
    "y_pred = model_ridgeCV.predict(scaled_X_test)\n",
    "RMSE = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "RMSE, mean_absolute_error(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We see that many weights are smaller, but not zero\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 5.84681185,  0.52142086,  0.71689997, -6.17948738,  3.75034058,\n",
       "       -1.36283352, -0.08571128,  0.08322815, -0.34893776,  2.16952446,\n",
       "       -0.47840838,  0.68527348,  0.63080799, -0.5950065 ,  0.61661989,\n",
       "       -0.31335495,  0.36499629,  0.03328145, -0.13652471])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"We see that many weights are smaller, but not zero\")\n",
    "model_ridgeCV.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lasso regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpha = 0.004968802520343365\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.5785146895301981, 0.4629188302693299)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LassoCV\n",
    "\n",
    "# it is trying 100 different alphas along regularization path epsilon\n",
    "model_lassoCV = LassoCV(eps = 0.001, n_alphas = 100, max_iter=int(1e4), cv=5)\n",
    "model_lassoCV.fit(scaled_X_train, y_train)\n",
    "print(f\"alpha = {model_lassoCV.alpha_}\")\n",
    "\n",
    "y_pred = model_lassoCV.predict(scaled_X_test)\n",
    "\n",
    "np.sqrt(mean_squared_error(y_test, y_pred)), mean_absolute_error(y_test, y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 5.19612354,  0.43037087,  0.29876351, -4.80417579,  3.46665205,\n",
       "       -0.40507212,  0.        ,  0.        ,  0.        ,  1.35260206,\n",
       "       -0.        ,  0.        ,  0.14879719, -0.        ,  0.        ,\n",
       "        0.        ,  0.09649665,  0.        ,  0.04353956])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we notice that many coefficients have been set to 0 using Lasso\n",
    "# it has selected some features for us \n",
    "model_lassoCV.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"max-width:66ch;\">\n",
    "\n",
    "### Elastic net\n",
    "\n",
    "Elastic net is a combination of both Ridge l2-regularization and Lasso l1-regularization. \n",
    "\n",
    "l1_ratio determines the ratio for $\\ell_1$ or $\\ell_2$ regularization.\n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L1 ratio: 1.0\n",
      "alpha 0.004968802520343365\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import ElasticNetCV\n",
    "\n",
    "# note that alpha here is lambda in the theory\n",
    "# l1_ratio is alpha in the theory\n",
    "model_elastic = ElasticNetCV(l1_ratio=[.1, .5, .7, .9, .95, .99, 1], eps = 0.001, n_alphas = 100, max_iter=int(1e4))\n",
    "model_elastic.fit(scaled_X_train, y_train)\n",
    "print(f\"L1 ratio: {model_elastic.l1_ratio_}\") # this would remove ridge and pick Lasso regression entirely\n",
    "print(f\"alpha {model_elastic.alpha_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.5785146895301981, 0.4629188302693299)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = model_elastic.predict(scaled_X_test)\n",
    "np.sqrt(mean_squared_error(y_test, y_pred)), mean_absolute_error(y_test, y_pred)\n",
    "# note that the result is same for Lasso regression which is expected as l1_ratio of 1 is same as using Lasso regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"max-width:66ch;\">\n",
    "\n",
    "## Summary\n",
    "\n",
    "In this lecture we've covered the concepts of regularizing linear models using ridge $\\ell_2$ and lasso $\\ell_1$ and finally elastic net which is a combination of both $\\ell_1$ and $\\ell_2$. The ratio is a hyperparameter that needs tuning, which can be done using k-fold cross validation.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #FFF; color: #212121; border-radius: 1px; width:22ch; box-shadow: rgba(0, 0, 0, 0.16) 0px 1px 4px; display: flex; justify-content: center; align-items: center;\">\n",
    "<div style=\"padding: 1.5em 0; width: 70%;\">\n",
    "    <h2 style=\"font-size: 1.2rem;\">Kokchun Giang</h2>\n",
    "    <a href=\"https://www.linkedin.com/in/kokchungiang/\" target=\"_blank\" style=\"display: flex; align-items: center; gap: .4em; color:#0A66C2;\">\n",
    "        <img src=\"https://content.linkedin.com/content/dam/me/business/en-us/amp/brand-site/v2/bg/LI-Bug.svg.original.svg\" width=\"20\"> \n",
    "        LinkedIn profile\n",
    "    </a>\n",
    "    <a href=\"https://github.com/kokchun/Portfolio-Kokchun-Giang\" target=\"_blank\" style=\"display: flex; align-items: center; gap: .4em; margin: 1em 0; color:#0A66C2;\">\n",
    "        <img src=\"https://github.githubassets.com/images/modules/logos_page/GitHub-Mark.png\" width=\"20\"> \n",
    "        Github portfolio\n",
    "    </a>\n",
    "    <span>AIgineer AB</span>\n",
    "<div>\n",
    "</div>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-part-1-handelsakademin-5RQamIMR-py3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
