# Week 17 - Resources

Note that the resource material is very extensive and you are not required to read them all. See it as a reference material, which you always can come back to. 

## Video guides :video_camera:
- [What is neural network - 3Blue1Brown](https://www.youtube.com/watch?v=aircAruvnKk)
- [Neural networks explained in 5 minutes - IBM](https://www.youtube.com/watch?v=jmmW0F0biz0)
- [Backpropagation - 3Blue1Brown](https://www.youtube.com/watch?v=Ilg3gGewQ5U&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&index=3)
- [Backpropagation calculus - 3Blue1Brown](https://www.youtube.com/watch?v=tIeHLnjs5U8&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&index=5)
- [How many hidden layers and nodes? - DigitalSreeni](https://www.youtube.com/watch?v=bqBRET7tbiQ)
- [Cross entropy - normalizednerd](https://www.youtube.com/watch?v=gIx974WtVb4)

Improving on gradient descent
- [Gradient descent with momentum - Andrew Ng](https://www.youtube.com/watch?v=k8fTYJPd3_I)
- [RMSProp optimization algorithm - Andrew Ng](https://www.youtube.com/watch?v=_e-LFe_igno)
- [Adam optimization algorithm - Andrew Ng](https://www.youtube.com/watch?v=JXQT_vxqwIs)

## Theory :book:
- [Neural networks - Michael Nielsen](http://neuralnetworksanddeeplearning.com/chap1.html)
- [Percecptron - wikipedia](https://en.wikipedia.org/wiki/Perceptron)
- [Universal approximator theorem - wikipedia](https://en.wikipedia.org/wiki/Universal_approximation_theorem)
- [Activation function - wikipedia](https://en.wikipedia.org/wiki/Activation_function)
- [Multilayer perceptron (MLP) - wikipedia](https://en.wikipedia.org/wiki/Multilayer_perceptron)
- [Rectifier (neural networks) - wikipedia](https://en.wikipedia.org/wiki/Rectifier_(neural_networks))
- [Biological neuron model - wikipedia](https://en.wikipedia.org/wiki/Biological_neuron_model)
- [Backpropagation - Michael Nielsen](http://neuralnetworksanddeeplearning.com/chap2.html)
- [Backpropagation - wikipedia](https://en.wikipedia.org/wiki/Backpropagation#Motivation)
- [Configure layers and nodes in NN - machinelearningmastery](https://machinelearningmastery.com/how-to-configure-the-number-of-layers-and-nodes-in-a-neural-network/)
- [Adam optimizer - wikipedia](https://en.wikipedia.org/wiki/Stochastic_gradient_descent#Adam)
- [Cross-entropy - wikipedia](https://en.wikipedia.org/wiki/Cross_entropy)
- [Cross-entropy - machinelearningmastery](https://machinelearningmastery.com/cross-entropy-for-machine-learning/)
- [Vanishing gradient problem - wikipedia](https://en.wikipedia.org/wiki/Vanishing_gradient_problem)
- [Probabilistic machine learning draft pp. 419-444 - Murphy (2022)](https://probml.github.io/pml-book/book1.html)
- [Softmax function - wikipedia](https://en.wikipedia.org/wiki/Softmax_function)
- [How many hidden layers/neurons - Gad (2018) towardsdatascience](https://towardsdatascience.com/beginners-ask-how-many-hidden-layers-neurons-to-use-in-artificial-neural-networks-51466afa0d3e)
- [Rule of thumb - Ranjan (2019) towardsdatascience](https://towardsdatascience.com/17-rules-of-thumb-for-building-a-neural-network-93356f9930af)
- [Convolution - wikipedia](https://en.wikipedia.org/wiki/Convolution)

**Keras**

- [Sequential model guide - TensorFlow](https://www.tensorflow.org/guide/keras/sequential_model)
- [InputLayer Keras - TensorFlow](https://www.tensorflow.org/api_docs/python/tf/keras/layers/InputLayer)
- [Dense layer Keras - TensorFlow](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense)
- [Visualize model training history - machinelearningmastery](https://machinelearningmastery.com/display-deep-learning-model-training-history-in-keras/)
- [EarlyStopping - Keras](https://keras.io/api/callbacks/early_stopping/)
- [None in model - stackoverflow](https://stackoverflow.com/questions/47240348/what-is-the-meaning-of-the-none-in-model-summary-of-keras)
- [Sparse categorical cross-entropy keras - TensorFlow](https://www.tensorflow.org/api_docs/python/tf/keras/losses/SparseCategoricalCrossentropy?version=nightly)
- [Classification metrics in Keras - machinelearningmastery](https://machinelearningmastery.com/custom-metrics-deep-learning-keras-python/)

**TensorFlow basics** 
to get understanding of how pure TensorFlow works
- [TensorFlow basics - TensorFlow](https://www.tensorflow.org/guide/basics)
- [Introduction to Tensors - TensorFlow](https://www.tensorflow.org/guide/tensor)
- [Variables - TensorFlow](https://www.tensorflow.org/guide/variable)
- [Training loops - TensorFlow](https://www.tensorflow.org/guide/basic_training_loops)

**Mathematical prerequisite**

- [Gradient - wikipedia svenska](<https://sv.wikipedia.org/wiki/Gradient_(matematik)>)
- [Partiell derivata - wikipedia svenska](https://sv.wikipedia.org/wiki/Partiell_derivata)

## Lecture notes :mortar_board:

- lecture theory 11-11.1
- lecture code 11

## Exercises :running:

- finish lab 1 
- exercise 5-6
